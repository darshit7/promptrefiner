{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Get Started","text":"<p> Promptrefiner </p> <p> </p> <p> Enhancing prompts with intelligent strategies for LLMs </p> <p> </p>"},{"location":"#welcome-to-promptrefiner","title":"\ud83d\ude80 Welcome to Promptrefiner","text":"<p>Helping you craft the perfect prompt for better LLM responses!</p> <p>PromptRefiner is a lightweight Python library that helps users write better prompts for Large Language Models (LLMs) with minimal configurations. Many users struggle to craft effective prompts that yield the desired results.  </p> <p>PromptRefiner takes user input, applies a selected strategy, and returns an improved prompt to get more specific and effective responses from LLMs (Large Language Models). It achieves this by leveraging an LLM to refine the user\u2019s prompt based on predefined strategies, making it easier to get high-quality responses.</p> <p>Whether you're using a prompt for GPT-4, Claude, Mistral, or any other LLM, PromptRefiner ensures your input is well-structured for the best possible output.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<p>\u2705 Supports 100+ LLM Clients \u2013 Works with OpenAI, Anthropic, Hugging Face, and more! \u2705 Highly Customizable \u2013 Use different LLM clients per strategy or a single client for all. \u2705 Command-Line First \u2013 Quickly refine prompts from the CLI for rapid experimentation. \u2705 Extensible \u2013 Developers can create their own custom prompt refinement strategies. \u2705 Seamless Integration \u2013 Works effortlessly in Python applications or scripts.  </p>"},{"location":"#installation","title":"\ud83d\udce5 Installation","text":"<p>Install PromptRefiner using pip:</p> <pre><code>pip install promptrefiner\n</code></pre>"},{"location":"#quick-start","title":"\u26a1 Quick Start","text":""},{"location":"#using-from-the-command-line","title":"\ud83d\udd27 Using from the Command Line","text":"<p>Before using <code>promptrefiner</code> in Python, make sure to set environment variables (Windows users should use set instead of export):  </p> <pre><code>export PREFINER_API_KEY=\"your-api-key-here\"\nexport PREFINER_MODEL=\"openai/gpt-4\"  # Change based on your LLM model\n</code></pre> <p>and there you go...  </p> <pre><code>promptrefiner --strategy persona \"Tell me about AI\"\n</code></pre>"},{"location":"#using-in-a-python-script","title":"\ud83d\udc0d Using in a Python Script","text":"<p>Note</p> <p>Make sure to set environment variables <code>PREFINER_API_KEY</code> and <code>PREFINER_MODEL</code> before using <code>PromptRefiner</code> in your python script.</p> <pre><code>from promptrefiner import PromptRefiner\n\nprompt_refiner = PromptRefiner(strategies=[\"persona\"])\nrefined_prompt = prompt_refiner.refine(\"Explain quantum mechanics.\")\nprint(refined_prompt)\n</code></pre>"},{"location":"#how-it-works","title":"\ud83d\udd0d How It Works","text":"<ol> <li>User provides a prompt (e.g., \"Tell me about AI\").</li> <li>Selects a strategy (e.g., \"verbose\" for a more detailed response).</li> <li>PromptRefiner applies a system prompt template for that strategy.</li> <li>Sends it to an LLM for refinement.</li> <li>Returns the improved prompt back to the user.</li> </ol> <p>\ud83d\ude80 Under the hood: Each strategy is backed by a system prompt template that guides the LLM to refine the user\u2019s input for better results.</p>"},{"location":"#why-use-promptrefiner","title":"\ud83e\udd14 Why Use PromptRefiner?","text":"<p>\ud83d\udd39 Improve prompt clarity &amp; effectiveness \u2013 Get sharper, more relevant responses. \ud83d\udd39 Save time \u2013 No need to manually tweak prompts for better results. \ud83d\udd39 Optimized for developers &amp; researchers \u2013 Quickly test different prompting strategies. \ud83d\udd39 Fine-tune for different LLMs \u2013 Customize strategies for specific AI models. \ud83d\udd39 Works for various use cases: </p> <ul> <li>Chatbots &amp; AI assistants</li> <li>Content generation &amp; summarization</li> <li>Data extraction from LLMs</li> <li>Code generation improvements</li> </ul>"},{"location":"#join-us-contribute","title":"\ud83d\ude80 Join Us &amp; Contribute!","text":"<p>We welcome contributors, feedback, and feature suggestions! \ud83d\ude80</p> <p>\ud83d\udccc GitHub Repo: darshit7/promptrefiner \ud83d\udccc Documentation: Promptrefiner \ud83d\udccc Report Issues &amp; Ideas: Coming Soon</p> <p>\ud83d\udc65 Want to improve PromptRefiner? Open a GitHub issue or contribute a pull request! \ud83d\udee0\ufe0f</p> <p>\ud83d\ude80 Refine your prompts. Supercharge your AI interactions. Try PromptRefiner today!</p>"},{"location":"contributing/","title":"Contributing to PromptRefiner","text":"<p>Thank you for your interest in contributing to PromptRefiner! \ud83d\ude80 This guide will help you set up a development environment and contribute effectively.</p>"},{"location":"contributing/#setting-up-the-development-environment","title":"\ud83d\udee0 Setting Up the Development Environment","text":""},{"location":"contributing/#1-clone-the-repository","title":"1\ufe0f\u20e3 Clone the Repository","text":"<pre><code>git clone https://github.com/darshit7/promptrefiner.git\ncd promptrefiner\n</code></pre>"},{"location":"contributing/#2-install-hatch-if-not-installed","title":"2\ufe0f\u20e3 Install Hatch (if not installed)","text":"<p>We use Hatch for dependency management and virtual environments. If you don\u2019t have it installed, run: <pre><code>pip install hatch\n</code></pre></p>"},{"location":"contributing/#3-create-and-activate-the-development-environment","title":"3\ufe0f\u20e3 Create and Activate the Development Environment","text":"<p>To set up your environment, run: <pre><code>hatch env create dev\nhatch shell dev\n</code></pre> This installs all dependencies listed under <code>[tool.hatch.envs.dev]</code> in <code>pyproject.toml</code>.</p>"},{"location":"contributing/#4-verify-installation","title":"4\ufe0f\u20e3 Verify Installation","text":"<p>To confirm everything is set up correctly, run: <pre><code>pytest  # Runs tests\nmypy .  # Runs type checks\nruff check .  # Runs linter\n</code></pre></p>"},{"location":"contributing/#making-contributions","title":"\ud83c\udfd7 Making Contributions","text":""},{"location":"contributing/#found-a-bug","title":"\ud83d\udc1b Found a Bug?","text":"<ul> <li>Open an issue describing the bug and steps to reproduce.</li> <li>If you already have a fix, feel free to submit a PR!</li> </ul>"},{"location":"contributing/#adding-a-feature","title":"\u2728 Adding a Feature?","text":"<ul> <li>Discuss your idea in an issue first.</li> <li>Follow best practices for writing clean, maintainable Python code.</li> </ul>"},{"location":"contributing/#writing-documentation","title":"\ud83d\udcdd Writing Documentation?","text":"<ul> <li>We use MkDocs with <code>mkdocs-material</code>. Run <code>mkdocs serve</code> locally to preview changes.</li> </ul>"},{"location":"contributing/#running-tests","title":"\ud83d\udd04 Running Tests","text":"<p>Before submitting a PR, ensure all tests pass: <pre><code>pytest\n</code></pre></p> <p>If adding new functionality, include appropriate tests.</p>"},{"location":"contributing/#submitting-a-pull-request","title":"\ud83d\ude80 Submitting a Pull Request","text":"<ol> <li>Fork the repository and create a new branch.</li> <li>Make your changes and commit them with a meaningful message.</li> <li>Push your branch and open a PR.</li> <li>Ensure your PR follows the style guide and passes all checks.</li> </ol>"},{"location":"contributing/#code-style-linting","title":"\ud83d\udee1 Code Style &amp; Linting","text":"<p>We enforce PEP8 and other best practices using <code>ruff</code>, <code>black</code>, and <code>mypy</code>. Before committing, format your code: <pre><code>black .\nruff check . --fix\nmypy .\n</code></pre></p>"},{"location":"contributing/#join-the-community","title":"\ud83d\ude4c Join the Community","text":"<p>We welcome contributions of all kinds! Whether it's code, documentation, or discussions, your input is valuable.</p> <p>Feel free to open an issue or join discussions to help improve PromptRefiner! \ud83d\ude80</p>"},{"location":"models/","title":"\ud83d\udccc Supported Models in PromptRefiner","text":"<p>Promptrefiner leverages LiteLLM under the hood, providing seamless integration with 100+ Large Language Models (LLMs) from top AI providers like OpenAI, Anthropic, TogetherAI, AI21, and more! \ud83d\ude80  </p> <p>Why does this matter? This means any model supported by LiteLLM is automatically supported by Promptrefiner. You can refine your prompts using the most advanced AI models available today.</p>"},{"location":"models/#supported-models","title":"\ud83c\udfaf Supported Models","text":"<p>Since Promptrefiner relies on LiteLLM, it supports all models listed here.</p> <p>To see the full list of supported models, run:</p> <pre><code>from litellm import model_list\nprint(model_list())\n</code></pre>"},{"location":"models/#how-model-api_key-selection-works","title":"\ud83c\udfaf How Model API_KEY Selection Works?","text":"<p>To interact with hosted LLM you will need an API_KEY, it's confidential and mostly provided to any application from environment variable. When you work with promptrefiner you don't need to worry about it, because we internally map <code>PREFINER_API_KEY</code> to the right API key automatically! We've designed it to make things easier by allowing a single environment variable:</p> <pre><code>export PREFINER_API_KEY=\"your_secret_key_here\"\n</code></pre> <p>Note</p> <p>But if you prefer provider-specific API keys, you can still use them! \u2705  </p>"},{"location":"models/#how-api-key-handling-works","title":"\u26a1 How API Key Handling Works?","text":"<p>Promptrefiner automatically detects the model provider and assigns the appropriate API key.</p> <p>\u2714 If using PREFINER_API_KEY \u2192 No extra setup needed! \u2714 If using provider-specific API keys \u2192 The library will detect and use them accordingly.  </p>"},{"location":"models/#example-usage","title":"\ud83d\udee0 Example Usage:","text":""},{"location":"models/#using-prefiner_api_key-recommended-for-simplicity","title":"Using PREFINER_API_KEY (Recommended for Simplicity)","text":"<pre><code>export PREFINER_API_KEY=\"your_openai_or_anthropic_key\"\n# If PREFINER_MODEL exported, default to openai/gpt-3.5-turbo\nexport PREFINER_MODEL=\"anthropic/claude-3.5\"\n</code></pre> <pre><code>from promptrefiner import PromptRefiner\n\nprompt_refiner = PromptRefiner(strategies=[\"persona\"])\nrefined_prompt = prompt_refiner.refine(\"Explain quantum mechanics.\")\nprint(refined_prompt)\n</code></pre>"},{"location":"models/#using-provider-specific-api-keys","title":"Using Provider-Specific API Keys","text":"<pre><code>export ANTHROPIC_API_KEY=\"your_anthropic_key\"\n# If PREFINER_MODEL exported, default to openai/gpt-3.5-turbo\nexport PREFINER_MODEL=\"anthropic/claude-3.5\"\n</code></pre> <pre><code>from promptrefiner import PromptRefiner\n\nprompt_refiner = PromptRefiner(strategies=[\"persona\"])\nrefined_prompt = prompt_refiner.refine(\"Explain quantum mechanics.\")\nprint(refined_prompt)\n</code></pre>"},{"location":"release_notes/","title":"Release Notes","text":""},{"location":"release_notes/#coming-soon","title":"\ud83d\udea7 Coming Soon!","text":"<p>The content will be available soon. \ud83d\ude80  </p> <p>If you have any suggestions or would like to contribute, feel free to check out our GitHub repository.  </p>"},{"location":"research/","title":"Related Research","text":""},{"location":"research/#research-on-prompt-techniques","title":"\ud83d\udcda Research on Prompt Techniques","text":"<p>Prompt engineering is an evolving field, and several research papers explore techniques that enhance interactions with language models. Below are some key papers that discuss various prompting strategies and their applications.</p> <p>We aim to incorporate insights from these research papers and resources like Prompt Engineering Guide into <code>promptrefiner</code>, aligning its prompt-enhancement strategies with state-of-the-art methodologies in prompt engineering.</p>"},{"location":"research/#key-research-papers","title":"\ud83d\udd0d Key Research Papers","text":"<ol> <li> <p>A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications Pranab Sahoo et al. (2024)    This paper provides a structured overview of prompt engineering methods, categorizing them by application and discussing their strengths and limitations.</p> </li> <li> <p>The Prompt Report: A Systematic Survey of Prompting Techniques Sander Schulhoff et al. (2024)    A comprehensive survey assembling a taxonomy of prompting techniques, analyzing their applications and providing a vocabulary of related terms.</p> </li> <li> <p>Prompt Design and Engineering: Introduction and Advanced Methods Xavier Amatriain (2024)    Introduces core concepts and advanced techniques in prompt design, including Chain-of-Thought and Reflection-based prompting.</p> </li> <li> <p>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT Jules White et al. (2023)    A pattern-based approach to prompt engineering, offering reusable solutions to common problems when interacting with LLMs.</p> </li> <li> <p>A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models Jindong Gu et al. (2023)    A deep dive into prompting techniques for multimodal models, summarizing their applications, challenges, and future research directions.</p> </li> </ol> <p>These references serve as valuable resources for understanding different prompt engineering techniques and their potential applications.</p>"},{"location":"strategies/","title":"Prompt Refiner Strategies","text":""},{"location":"strategies/#available-strategies","title":"Available Strategies","text":"<p>The following table lists the available prompt enhancement strategies in PromptRefiner, along with their aliases and descriptions.</p> Strategy Alias Description <code>persona</code> <code>per</code> Returns an optimized Persona-Based prompting instruction for LLMs. <code>fewshot</code> <code>fs</code> Refines a prompt by following the Few-Shot prompt technique. <code>chainofthought</code> <code>cot</code> Returns an optimized Chain-of-Thought (CoT) prompting instruction for LLMs. <code>selfconsist</code> <code>sc</code> Returns an optimized Self-Consistency prompting instruction for LLMs. <code>recref</code> <code>rcr</code> Returns an optimized Recursive Critique &amp; Refinement prompting instruction for LLMs. <p>Each strategy enhances prompt quality by applying a specific prompting technique, ensuring better interaction with large language models (LLMs).</p> <p>We frequently onboard new techniques. If you have suggestions or would like to contribute, please refer to the Contributing Guidelines.</p>"},{"location":"strategies/#coming-soon","title":"\ud83d\udea7 Coming Soon!","text":"<p>The detailed content will be available soon. \ud83d\ude80</p>"},{"location":"api/base_strategy/","title":"Base Strategy","text":""},{"location":"api/base_strategy/#promptrefiner.base.BaseStrategy","title":"<code>BaseStrategy(llm_client=None, api_key=None, model=None, temperature=None, **kwargs)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all prompt refinement strategies.</p> <p>Initializes with OpenAI API credentials (defaults to global config).</p> Source code in <code>promptrefiner/base.py</code> <pre><code>def __init__(\n    self,\n    llm_client=None,\n    api_key=None,\n    model=None,\n    temperature=None,\n    **kwargs,\n):\n    \"\"\"\n    Initializes with OpenAI API credentials (defaults to global config).\n    \"\"\"\n    if llm_client:\n        self.llm_client = llm_client\n    else:\n        config = load_config(api_key, model, temperature)\n        self.llm_client = get_llm_client(\n            config[\"api_key\"], config[\"model\"], temperature, **kwargs\n        )\n</code></pre>"},{"location":"api/base_strategy/#promptrefiner.base.BaseStrategy.get_system_prompt","title":"<code>get_system_prompt()</code>  <code>abstractmethod</code>","text":"<p>Each strategy must define its own system prompt.</p> Source code in <code>promptrefiner/base.py</code> <pre><code>@abstractmethod\ndef get_system_prompt(self) -&gt; str:  # pragma: no cover\n    \"\"\"Each strategy must define its own system prompt.\"\"\"\n    pass\n</code></pre>"},{"location":"api/base_strategy/#promptrefiner.base.BaseStrategy.refine","title":"<code>refine(prompt)</code>","text":"<p>Refine a prompt using OpenAI API, applying strategy-specific system instructions.</p> Source code in <code>promptrefiner/base.py</code> <pre><code>def refine(self, prompt: str) -&gt; str:\n    \"\"\"Refine a prompt using OpenAI API, applying strategy-specific system instructions.\"\"\"\n    return self.llm_client(self.get_system_prompt(), prompt)\n</code></pre>"},{"location":"api/client_factory/","title":"LLM Client","text":"<p>Implements <code>get_llm_client</code> method to configure <code>litellm.completion</code> to communicate with user provided LLM client.</p>"},{"location":"api/client_factory/#promptrefiner.client_factory.get_llm_client","title":"<code>get_llm_client(api_key, model, temperature, **kwargs)</code>","text":"<p>Generate a lambda function around <code>litellm.completion</code> to be called from <code>PromptRefiner.refine</code>.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key to access model.</p> required <code>model</code> <code>str</code> <p>model name to use for refining prompt.</p> required <code>temperature</code> <code>float</code> <p>Temperature for model.</p> required <code>**kwargs</code> <p>Extra arguments to feed into model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A lambda function, which takes <code>system_prompt</code> and <code>user_prompt</code> as an argument and retunrs refined prompt on call.</p> Source code in <code>promptrefiner/client_factory.py</code> <pre><code>def get_llm_client(api_key: str, model: str, temperature: float, **kwargs) -&gt; Callable:\n    \"\"\"\n    Generate a lambda function around ` litellm.completion` to be called\n    from `PromptRefiner.refine`.\n\n    Args:\n        api_key (str): API key to access model.\n        model (str): model name to use for refining prompt.\n        temperature (float): Temperature for model.\n        **kwargs: Extra arguments to feed into model.\n\n    Returns:\n        A lambda function, which takes `system_prompt` and `user_prompt`\n            as an argument and retunrs refined prompt on call.\n    \"\"\"\n\n    # Identify which environment variable `litellm` expects for the chosen model\n    provider = model.split(\"/\")[0]\n    expected_env_var = MODEL_API_KEY_MAP.get(provider)\n\n    # Set the API key dynamically for the expected environment variable\n    if expected_env_var and api_key:\n        os.environ[expected_env_var] = api_key\n\n    # Return a function that calls litellm\n    return lambda system_prompt, user_prompt: litellm.completion(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n        temperature=temperature,\n        **kwargs,\n    )[\"choices\"][0][\"message\"][\"content\"]\n</code></pre>"},{"location":"api/refiner/","title":"Refiner","text":"<p>This module defines the main <code>PromptRefiner</code> class and associated functions for modifying and enhancing prompts using various strategies.</p>"},{"location":"api/refiner/#promptrefiner.refiner.PromptRefiner","title":"<code>PromptRefiner(strategies)</code>","text":"<p>               Bases: <code>object</code></p> <p>A class for refining prompt using multiple strategies.</p> <p>This class applies a series of transformations to enhance a given input prompt. It supports multiple strategies, such as \"few_shot\" and \"meta\".</p> <p>Attributes:</p> Name Type Description <code>strategies</code> <code>list[str | BaseStrategy]</code> <p>A list of strategies to be applied to the prompt.</p> <p>Initialize the <code>PromptRefiner</code> with list of strategies.</p> <p>Parameters:</p> Name Type Description Default <code>strategies</code> <code>list</code> <p>A list of supported strategies.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If provided strategy is not supported.</p> Source code in <code>promptrefiner/refiner.py</code> <pre><code>def __init__(self, strategies: list):\n    \"\"\"\n    Initialize the `PromptRefiner` with list of strategies.\n\n    Args:\n        strategies (list): A list of supported strategies.\n\n    Raises:\n        ValueError: If provided strategy is not supported.\n    \"\"\"\n    if not strategies:\n        raise ValueError(\"At least one strategy must be provided.\")\n\n    self.strategies = []\n    for strategy in strategies:\n        if isinstance(strategy, str):\n            normalized_name = next(\n                (\n                    name\n                    for name, data in STRATEGY_MAP.items()\n                    if strategy.lower() in [name] + data[\"aliases\"]\n                ),\n                None,\n            )\n            if not normalized_name:\n                raise ValueError(f\"Unsupported strategy: {strategy}\")\n            strategy_class = STRATEGY_MAP[normalized_name][\"class_\"]\n            self.strategies.append(strategy_class())\n        elif isinstance(strategy, type) and issubclass(strategy, BaseStrategy):\n            self.strategies.append(strategy())  # Instantiate class\n        elif isinstance(strategy, BaseStrategy):\n            self.strategies.append(strategy)  # Already instantiated\n        else:\n            logger.error(f\"Invalid strategy: {strategy}\")\n            raise ValueError(f\"Invalid strategy: {strategy}\")\n    logger.info(f\"PromptRefiner initialized with strategies: {self.strategies}\")\n</code></pre>"},{"location":"api/refiner/#promptrefiner.refiner.PromptRefiner.refine","title":"<code>refine(prompt)</code>","text":"<p>Applies all strategies in sequence and returns refined prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The refined prompt after applying strategies.</p> Source code in <code>promptrefiner/refiner.py</code> <pre><code>def refine(self, prompt: str) -&gt; str:\n    \"\"\"\n    Applies all strategies in sequence and returns refined prompt.\n\n    Args:\n        prompt (str): The input prompt.\n\n    Returns:\n        str: The refined prompt after applying strategies.\n    \"\"\"\n    for strategy in self.strategies:\n        prompt = strategy.refine(prompt)\n    return prompt\n</code></pre>"},{"location":"api/setup/","title":"Loading Configurations","text":"<p>Loads configurations realted to LLM model, API keys and temprture to be used.</p>"},{"location":"api/setup/#promptrefiner.config.load_config","title":"<code>load_config(api_key=None, model=None, temperature=None)</code>","text":"<p>If <code>api_key</code>, <code>model</code> or <code>temperature</code> provided while loading config uses it, otherwise loads it from environment varilable.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API Key to connect with model</p> <code>None</code> <code>model</code> <code>str</code> <p>model name in <code>litellm</code> supported format.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>temperature parameter for model.</p> <code>None</code> <p>Returns:</p> Type Description <p>Dictionary containing <code>api_key</code>, <code>model</code> and <code>temperature</code></p> Source code in <code>promptrefiner/config.py</code> <pre><code>def load_config(api_key=None, model=None, temperature=None):\n    \"\"\"\n    If `api_key`, `model` or `temperature` provided while loading config uses it,\n    otherwise loads it from environment varilable.\n\n    Args:\n        api_key (str): API Key to connect with model\n        model (str): model name in `litellm` supported format.\n        temperature (float): temperature parameter for model.\n\n    Returns:\n        Dictionary containing `api_key`, `model` and `temperature`\n    \"\"\"\n    api_key = api_key or os.getenv(\"PREFINER_API_KEY\")\n    model = model or os.getenv(\"PREFINER_MODEL\", \"openai/gpt-3.5-turbo\")\n    temperature = temperature or float(os.getenv(\"PREFINER_TEMP\", 0))\n\n    return {\"api_key\": api_key, \"model\": model, \"temperature\": temperature}\n</code></pre>"},{"location":"api/strategies/few_shot/","title":"Few Shot Strategy","text":""},{"location":"api/strategies/few_shot/#promptrefiner.strategies.few_shot.FewShot","title":"<code>FewShot(llm_client=None, api_key=None, model=None, temperature=None, **kwargs)</code>","text":"<p>               Bases: <code>BaseStrategy</code></p> <p>Refines a prompt by following few shot prompt technique.</p> Source code in <code>promptrefiner/base.py</code> <pre><code>def __init__(\n    self,\n    llm_client=None,\n    api_key=None,\n    model=None,\n    temperature=None,\n    **kwargs,\n):\n    \"\"\"\n    Initializes with OpenAI API credentials (defaults to global config).\n    \"\"\"\n    if llm_client:\n        self.llm_client = llm_client\n    else:\n        config = load_config(api_key, model, temperature)\n        self.llm_client = get_llm_client(\n            config[\"api_key\"], config[\"model\"], temperature, **kwargs\n        )\n</code></pre>"}]}